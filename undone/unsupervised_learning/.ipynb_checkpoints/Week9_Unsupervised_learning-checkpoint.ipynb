{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7973b12c",
   "metadata": {},
   "source": [
    "# Кластеризация\n",
    "\n",
    "На этом семинаре мы займёмся задачей кластеризации векторов, интерпретацией полученных результатов. В частности, посмотрим на задачу выделения отдельных видов товаров в категории объявлений как на задачу кластеризации.\n",
    "\n",
    "Нам предстоит: \n",
    "\n",
    "* посмотреть на подготовку векторов к кластеризации,\n",
    "* реализовать алгоритм KMeans и попробовать разные варианты его инициализации,\n",
    "* изучить примеры интерпретации полученных кластеров,\n",
    "* ознакомиться с подбором параметров DBSCAN,\n",
    "* решить задачу разделения категории на виды товаров."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e06fec",
   "metadata": {},
   "source": [
    "## Задача анализа заголовков в категории «Товары для школы»\n",
    "Цель — проанализировать, какие товары продаются в данной категории и какие заголовки там бывают для последующего использования этой информации при решении уже какой-то конкретной задачи.\n",
    "\n",
    "Мы имеем дело с текстовыми данными — чтобы кластеризовать их, нам необходимы вектора. Одними из самых простых методов получения векторов, пригодных для использования в реальных задачах, являются TfidfVectorizer или Word2Vec.\n",
    "\n",
    "В этом шаге мы воспользуемся моделью Word2Vec, предполагая, что она лучше подходит для смыслового анализа слов.\n",
    "\n",
    "Пайплайн получения векторов для заголовков из семинара:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62bcb87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim==4.2.0\n",
      "  Downloading gensim-4.2.0-cp310-cp310-macosx_10_9_universal2.whl (24.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.4/24.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /Users/daniilsobolev/anaconda3/lib/python3.10/site-packages (from gensim==4.2.0) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/daniilsobolev/anaconda3/lib/python3.10/site-packages (from gensim==4.2.0) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/daniilsobolev/anaconda3/lib/python3.10/site-packages (from gensim==4.2.0) (1.26.3)\n",
      "Installing collected packages: gensim\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.3.0\n",
      "    Uninstalling gensim-4.3.0:\n",
      "      Successfully uninstalled gensim-4.3.0\n",
      "Successfully installed gensim-4.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim==4.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42856fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib import request\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc35ba8",
   "metadata": {},
   "source": [
    "Данные можно [скачать](https://stepik.org/media/attachments/lesson/535633/titles_for_clustering_original.csv)\n",
    "    или загрузить по ссылке: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90f80d49",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Downloads/unsupervised_learning/titles_for_clustering_original.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDownloads/unsupervised_learning/titles_for_clustering_original.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Downloads/unsupervised_learning/titles_for_clustering_original.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Downloads/unsupervised_learning/titles_for_clustering_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0445e6df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2876cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3183d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a044be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# предобработка\n",
    "def preprocessing(text):\n",
    "    \"\"\"Делаем предобработку.\n",
    "    \n",
    "    - приводим к нижнему регистру\n",
    "    - удаляем пунктуацию и цифры\n",
    "    - удаляем английские слова: не имеют большой значимости для данной задачи\n",
    "    :return: строка обработанного текста\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^А-я]', ' ', text.lower())\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "data['title'] = data['title'].apply(preprocessing)\n",
    "# удаляем заголовки, которые стали пустыми после предобработки\n",
    "mask = data['title'].apply(lambda x: len(x.split()) > 0)\n",
    "data = data[mask]\n",
    "\n",
    "# data.drop_duplicates('title', inplace=True) - опционально, зависит от цели кластеризации\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9045ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1553a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучение Word2Vec\n",
    "w2v_corpus = [x.split() for x in data['title']]\n",
    "\n",
    "w2v_model = Word2Vec(min_count=0, vector_size=100, hs=1, window=1, seed=0)\n",
    "w2v_model.build_vocab(w2v_corpus)\n",
    "w2v_model.train(w2v_corpus, total_examples=w2v_model.corpus_count, epochs=w2v_model.epochs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4477a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv['учебник']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(w2v_model.wv['учебник'] - w2v_model.wv['книга'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167f0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получение векторов заголовков из векторов слов\n",
    "w2v_embeddings = []\n",
    "for title_words in w2v_corpus:\n",
    "    # не забываем, что некоторые слова могут не быть в словаре Word2Vec из-за выставленного min_count\n",
    "    title_word_embs = [\n",
    "        w2v_model.wv[word] \n",
    "        for word in title_words \n",
    "        if word in w2v_model.wv.key_to_index\n",
    "    ]\n",
    "    title_emb = sum(title_word_embs) / len(title_word_embs)\n",
    "    w2v_embeddings.append(title_emb)\n",
    "w2v_embeddings = np.vstack(w2v_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f94fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9413b2de",
   "metadata": {},
   "source": [
    "## Визуализация кластеров\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43faa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a00aa94",
   "metadata": {},
   "source": [
    "Часто используются два базовых метода анализа получившихся кластеров:\n",
    "\n",
    "* Посмотреть на несколько ближайших объектов до каждого центроида, исходя из этого оценить содержимое всего кластера.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2b04c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=19)\n",
    "kmeans.fit(w2v_embeddings)\n",
    "\n",
    "top = 5\n",
    "clusters = kmeans.labels_\n",
    "dist_to_centers = euclidean_distances(kmeans.cluster_centers_, w2v_embeddings)\n",
    "top_most_similar = np.argsort(dist_to_centers)[:, :top]\n",
    "\n",
    "for clt in np.unique(clusters):\n",
    "    ids = top_most_similar[clt]\n",
    "    most_similar_titles = '\\n'.join(data.iloc[ids]['title'])\n",
    "    print(f'Cluster {clt}:\\n{most_similar_titles}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2452bc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f8a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# следует реализовать данный вид инициализации\n",
    "# без нее тесты скорее всего не пройдут\n",
    "def k_plus_plus(X: np.ndarray, k: int, random_state: int = 27) -> np.ndarray:\n",
    "    \"\"\"Инициализация центроидов алгоритмом k-means++.\n",
    "\n",
    "    :param X: исходная выборка\n",
    "    :param k: количество кластеров\n",
    "    :return: набор центроидов в одном np.array\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class KMeansCustom:\n",
    "    def __init__(self, n_clusters=8, tol=0.0001, max_iter=300, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X):\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        # инициализируем центры кластеров\n",
    "        # centers.shape = (n_clusters, n_features)\n",
    "        centers = ...\n",
    "\n",
    "        for n_iter in range(self.max_iter):\n",
    "            # считаем расстояние от точек из X до центроидов\n",
    "            distances = ...\n",
    "            # определяем метки как индекс ближайшего для каждой точки центроида  \n",
    "            labels = ...\n",
    "\n",
    "            old_centers = centers.copy()\n",
    "            for c in range(self.n_clusters):\n",
    "                # пересчитываем центроид \n",
    "                # новый центроид есть среднее точек X с меткой рассматриваемого центроида\n",
    "                centers[c, :] = ...\n",
    "\n",
    "            # записываем условие сходимости\n",
    "            # норма Фробениуса разности центров кластеров двух последовательных итераций < tol\n",
    "            if ... :\n",
    "                break\n",
    "\n",
    "        # cчитаем инерцию\n",
    "        # сумма квадратов расстояний от точек до их ближайших центров кластеров\n",
    "        inertia = ...\n",
    "\n",
    "        self.cluster_centers_ = centers\n",
    "        self.labels_ = labels\n",
    "        self.inertia_ = inertia\n",
    "        self.n_iter_ = n_iter\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        # определяем метку для каждого элемента X на основании обученных центров кластеров \n",
    "        distances = ...\n",
    "        labels = ...\n",
    "        return labels\n",
    "\n",
    "    def fit_predict(self, X):\n",
    "        return self.fit(X).labels_\n",
    "\n",
    "\n",
    "kmeans = KMeansCustom(n_clusters=19)\n",
    "kmeans.fit(w2v_embeddings)\n",
    "\n",
    "top = 5\n",
    "clusters = kmeans.labels_\n",
    "dist_to_centers = euclidean_distances(kmeans.cluster_centers_, w2v_embeddings)\n",
    "top_most_similar = np.argsort(dist_to_centers)[:, :top]\n",
    "\n",
    "for clt in np.unique(clusters):\n",
    "    ids = top_most_similar[clt]\n",
    "    most_similar_titles = '\\n'.join(data.iloc[ids]['title'])\n",
    "    print(f'Cluster {clt}:\\n{most_similar_titles}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f66002",
   "metadata": {},
   "source": [
    "* Визуализация кластеров в 2D с помощью TSNE — широко распространённый метод визуализации данных высокой размерности. В случае очень большой размерности признакового пространства имеет смысл предварительно уменьшить размерность, например, с помощью PCA, для ускорения вычислений и устранения возможного шума."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ab8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# для каждого кластера сэмплируем не более 20 точек для ускорения TSNE и наглядности картинки\n",
    "sample_i = []\n",
    "for clt in np.unique(clusters):\n",
    "    cluster_mask = clusters == clt\n",
    "    sample_i += random.sample(list(np.where(cluster_mask)[0]), min(20, cluster_mask.sum()))\n",
    "\n",
    "# берем слайс по sample_i\n",
    "sample_embeddings = w2v_embeddings[sample_i]\n",
    "sample_clusters = clusters[sample_i].reshape(-1, 1)\n",
    "sample_texts = data['title'].values[sample_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучаем и применяем TSNE\n",
    "tsne_model = TSNE(perplexity=50, metric='cosine', init='pca', n_iter=2500, random_state=17)\n",
    "reduced_embeddings = tsne_model.fit_transform(sample_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_annotate_thr=0.3\n",
    "plt.figure(figsize=(18, 18)) \n",
    "colors = cm.nipy_spectral(sample_clusters.astype(float) / len(np.unique(sample_clusters)))\n",
    "for i, (x, y) in enumerate(reduced_embeddings):\n",
    "    if np.random.rand() > not_annotate_thr:\n",
    "        plt.scatter(x, y, c=colors[i])\n",
    "        plt.annotate(sample_texts[i],\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom', \n",
    "                     size=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7049ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.where(sample_texts == 'учебник')\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7462e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299b8ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embeddings[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19354a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86b07f00",
   "metadata": {},
   "source": [
    "[Почему так](https://datascience.stackexchange.com/questions/19025/t-sne-why-equal-data-values-are-visually-not-close)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e663ae97",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DBSCAN: краткий обзор\n",
    "\n",
    "Одним из наиболее часто используемых на практике методов кластеризации при неизвестном количестве кластеров является [DBSCAN](https://ru.wikipedia.org/wiki/DBSCAN)— метод кластеризации, основанный на плотности. \n",
    "\n",
    "Метод не применим к данным с большой разницей в плотности кластеров.\n",
    "\n",
    "Основные преимущества:\n",
    "\n",
    "1. Не требует априорного знания о необходимом количестве кластеров;\n",
    "1. Выделяет кластеры произвольной формы;\n",
    "1. Устойчив к выбросам, выделяет шум.\n",
    "\n",
    "Подбор параметров\n",
    "\n",
    "Вся сложность применения метода заключается в подборе двух основных параметров: `min_samples` и `eps`\n",
    "\n",
    "1. `min_samples` параметр определяет сколько объектов должно содержаться в `eps` окрестности выбранного объекта, чтобы этот объект считалась внутрикластерным. Имеет смысл брать только значения ≥3, на практике часто бывает достаточно ограничить поиски значениями от 3 до 9, но это носит лишь рекомендательный характер и не отрицает наличия оптимальной пары с `eps` вне данного отрезка\n",
    "1. `eps` параметр определяет размер окрестности. Чтобы больше определиться с областью поиска, для начала нужно выбрать функцию расстояния, так как для ненормированных векторов значения `euclidean_distances` и `cosine_distances` скорее всего лежат в разных диапазонах\n",
    "\n",
    "Далее можно воспользоваться двумя методами подбора: \n",
    "* Метод колена: для выбранного min_samples определяется подходящий eps\n",
    "\n",
    "*Идея*: для каждого объекта выборки считается расстояние до n-го соседа, где n выбирается равным `min_samples` . После этого расстояния сортируются и отрисовываются. В качестве `eps` берётся значение расстояния в точке с максимальной кривизной, предполагая, что точки с сильно растущим расстоянием до n-го соседа являются выбросами.\n",
    "\n",
    "Для поиска оптимальной точки можно воспользоваться библиотекой [kneed](https://kneed.readthedocs.io/en/stable/index.html) и ее классом [KneeLocator](https://kneed.readthedocs.io/en/stable/api.html#kneelocator). Суть его заключается в построении интерполирующей функции по заданным точкам (часто из класса полиномиальных функций) и выбора точки (среди заданных) с максимальным значением кривизны полученного интерполянта. В общепринятой формулировке метода колена значения расстояний располагаются в порядке возрастания, следовательно, имеем выпуклую кривую `curve='convex'` с порядком обхода по возрастанию `direction='increasing'`, параметр `interp_method='interp1d'` or `'polynomial'` отвечает методу интерполяции точек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f4e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e252f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad7ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_embeddings_sample = w2v_embeddings[::5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b55a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pairwise_distances(w2v_embeddings_sample)\n",
    "distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab60a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c15dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_distances = np.sort(distances, axis=1)\n",
    "sorted_nth_distanses = np.sort(sorted_distances[:, n - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb6eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "knee = KneeLocator(np.arange(len(sorted_distances)), \n",
    "                   sorted_nth_distanses, \n",
    "                   curve='convex', \n",
    "                   direction='increasing', \n",
    "                   interp_method='polynomial')\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "knee.plot_knee()\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "print(sorted_nth_distanses[knee.knee])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b330142b",
   "metadata": {},
   "source": [
    "В качестве параметров для `DBSCAN` выбираются значения `eps=0.84`, `min_samples=7`, `metric='euclidean'`.\n",
    "\n",
    "На практике бывает полезно перебрать `eps` в небольшой окрестности найденного оптимального значения из-за зависимости оптимума от выбранного метода и степени интерполяции, попробовать разную интерполяцию и разные значения для n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81501784",
   "metadata": {},
   "source": [
    "* Метод силуэта: \n",
    "\n",
    "*Идея*: перебирая по сетке значения для `min_samples` и `eps` , выбрать те, которые приводят к наибольшему значению [силуэт-метрики](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134067c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "for eps in np.linspace(0.05, 0.5, 10):\n",
    "    for ms in [5, 7, 9]:\n",
    "        clustering = DBSCAN(eps=eps, min_samples=ms, metric='euclidean', n_jobs=-1)\n",
    "        clusters = clustering.fit_predict(w2v_embeddings_sample)\n",
    "        score = silhouette_score(w2v_embeddings_sample, clusters)\n",
    "        print(f'eps = {eps:0.3f}, ms = {ms}: {score:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231110f",
   "metadata": {},
   "source": [
    "В домашнем задании будет два пункта:\n",
    "\n",
    "* Реализовать KMeans\n",
    "* Выполнить кластеризацию и разбиение заданной категории на виды товаров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2d79b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
